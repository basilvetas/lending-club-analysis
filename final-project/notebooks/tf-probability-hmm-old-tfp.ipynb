{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to use new TF HMM code with edward:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of Edward that we have been using so far uses an old version of TF, which includes Probability as a submodule tf.contrib.Distribution, and this submodule doesn't contain HMM.\n",
    "\n",
    "Yet the new version of TF Probability (**release 0.5**, and requires TF **1.12**), which is now a standalone module, does contain HMM. The goal of this notebook is to use that and wrap it as an edward Random Variable to do inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** The new TFP actually also contains the newest verion of Edward, as Edward2. But they haven't ported everything from Edward to Edward2 yet, in particular not the inference. We probably want to try to use only the new HMM from the new TF and use it in our (old) version of Edward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow.contrib.distributions as tfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting HMM from new TFP to our version of edward and tf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, abspath\n",
    "import sys\n",
    "sys.path.append(join(abspath('.'), '../utils'))\n",
    "from tf.tf_hidden_markov_model import HiddenMarkovModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example, from tfp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_distribution = tfd.Categorical(probs=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a cold day has a 30% chance of being followed by a hot day and a hot day has a 20% chance of being followed by a cold day.\n",
    "\n",
    "We can model this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
    "                                                 [0.2, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose additionally that on each day the temperature is normally distributed with mean and standard deviation 0 and 5 on a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
    "\n",
    "We can model this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these distributions into a single week long hidden Markov model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HiddenMarkovModel(\n",
    "    initial_distribution=initial_distribution,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=observation_distribution,\n",
    "    num_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"HiddenMarkovModel/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"HiddenMarkovModel/mean/Normal/mean/mul:0\", shape=(2,), dtype=float32)\n",
      "[2.9999998 5.9999995 7.4999995 8.25      8.625001  8.812501  8.90625  ]\n",
      "Tensor(\"HiddenMarkovModel/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"zeros:0\", shape=(7,), dtype=float32)\n",
      "-20.120832\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # The expected temperatures for each day are given by:\n",
    "    print(sess.run(model.mean()))  # shape [7], elements approach 9.0\n",
    "    # The log pdf of a week of temperature 0 is:\n",
    "    print(sess.run(model.log_prob(tf.zeros(shape=[7]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import Categorical, Dirichlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from issue \n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(probs=tf.fill([n_hidden], 1.0 / n_hidden))\n",
    "\n",
    "# transition matrix\n",
    "T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "transition_distribution = Categorical(probs=T)\n",
    "\n",
    "# emission matrix\n",
    "E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0)\n",
    "emission_distribution = Categorical(probs=E)\n",
    "\n",
    "model = HiddenMarkovModel(\n",
    "    initial_distribution=x_0,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=emission_distribution,\n",
    "    num_steps=chain_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_graph_parents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-43a5e4200dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                          emission_distribution: qe},\n\u001b[1;32m     16\u001b[0m                         {model: y_data})\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/inference.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, variables, use_coordinator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mPassed\u001b[0m \u001b[0minto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, n_samples, kl_scaling, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKLqp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/variational_inference.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, optimizer, var_list, use_prettytensor, global_step, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36mbuild_loss_and_gradients\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0;31m#    return build_score_entropy_loss_and_gradients(self, var_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_score_rb_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36mbuild_score_rb_loss_and_gradients\u001b[0;34m(inference, var_list)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         \u001b[0mx_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_swap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m         p_log_probs[s][x] = tf.reduce_sum(\n\u001b[1;32m   1078\u001b[0m             inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(org_instance, dict_swap, scope, replace_itself, copy_q, copy_parent_rvs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# Create new random variable with copied arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m       \u001b[0mnew_rv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;31m# Handle case where parameters are copied under absolute name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/Columbia/Cours/Semestre 3/COMS 6998 Prob Programming/Project/lending-club-analysis/final-project/notebooks/../utils/tf_hidden_markov_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHiddenMarkovModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_HiddenMarkovModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mRandomVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/models/random_variable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'collections'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/Columbia/Cours/Semestre 3/COMS 6998 Prob Programming/Project/lending-club-analysis/final-project/notebooks/../utils/tf_hidden_markov_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_distribution, transition_distribution, observation_distribution, num_steps, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m     with tf.name_scope(name=name, values=(\n\u001b[1;32m    167\u001b[0m         \u001b[0minitial_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_parents\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtransition_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_parents\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         observation_distribution._graph_parents)) as name:\n\u001b[1;32m    170\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runtime_assertions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_graph_parents'"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "q_0 = Categorical(probs=tf.nn.softmax(tf.Variable(tf.ones(n_hidden), expected_shape=n_hidden)))\n",
    "qt = Categorical(probs=tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0))\n",
    "qe = Categorical(probs=tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0))\n",
    "qm = HiddenMarkovModel(q_0, qt, qe, chain_len)\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    inference = ed.KLqp({x_0: q_0,\n",
    "                         transition_distribution: qt,\n",
    "                         emission_distribution: qe},\n",
    "                        {model: y_data})\n",
    "    inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dirichlet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bbf49809a759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpi_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dirichlet' is not defined"
     ]
    }
   ],
   "source": [
    "# from issue \n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "pi_0 = Dirichlet(tf.ones(n_hidden))\n",
    "x_0 = Categorical(probs=pi_0)\n",
    "\n",
    "# transition matrix\n",
    "# pi_T = [Dirichlet(tf.ones(n_hidden)) for i in range(n_hidden)]\n",
    "pi_T = Dirichlet(tf.ones([n_hidden, n_hidden]))\n",
    "# T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "transition_distribution = Categorical(probs=pi_T)\n",
    "\n",
    "# emission matrix\n",
    "# E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0)\n",
    "pi_E = Dirichlet(tf.ones([n_hidden, n_obs]))\n",
    "emission_distribution = Categorical(probs=pi_E)\n",
    "\n",
    "model = HiddenMarkovModel(\n",
    "    initial_distribution=x_0,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=emission_distribution,\n",
    "    num_steps=chain_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: adding sample_shape to x_0, transmission_distribution, emission_distribution, and model, doesn't fail but doesn't converge either..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "qpi_0 = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden), name=\"q_0/concentration\")))\n",
    "qpi_T = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_hidden]), name=\"qt/concentration\")))\n",
    "qpi_E = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_obs]), name=\"qe/concentration\")))\n",
    "\n",
    "# qm = HiddenMarkovModel(q_0, qt, qe, chain_len) # not necessary\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp({pi_0: qpi_0,\n",
    "                         pi_T: qpi_T,\n",
    "                         pi_E: qpi_E},\n",
    "                        {model: y_data},)\n",
    "    inference.run(n_iter=5000, logdir='./testlog')\n",
    "    inferred_pi0, inferred_pi_T, inferred_pi_E = sess.run([qpi_0.mean(), qpi_T.mean(), qpi_E.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67318153, 0.1437307 , 0.18308769],\n",
       "       [0.19555463, 0.57955325, 0.2248921 ],\n",
       "       [0.2137069 , 0.27166   , 0.5146331 ]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12495308, 0.6211583 , 0.25388864],\n",
       "       [0.56491506, 0.1938043 , 0.2412807 ],\n",
       "       [0.28481784, 0.32066643, 0.39451572]], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ed.RandomVariable 'Categorical_65/' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transition_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Categorical_65/logits/Log:0' shape=(3, 3) dtype=float32>,\n",
       " <tf.Tensor 'Dirichlet_111/sample/Reshape:0' shape=(3, 3) dtype=float32>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_distribution._graph_parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM inference:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- run E more than M\n",
    "- pass results from E as data in M??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inference_14/sample_32/HiddenMarkovModel_4/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"data_14/Variable/read:0\", shape=(30,), dtype=int32)\n",
      "Tensor(\"inference_15/sample_33/HiddenMarkovModel_4/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"data_15/Variable/read:0\", shape=(30,), dtype=int32)\n",
      "0  / 1000 done\n",
      "100  / 1000 done\n",
      "200  / 1000 done\n",
      "300  / 1000 done\n",
      "400  / 1000 done\n",
      "500  / 1000 done\n",
      "600  / 1000 done\n",
      "700  / 1000 done\n",
      "800  / 1000 done\n",
      "900  / 1000 done\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qpi_0 = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden), name=\"q_0/concentration\")))\n",
    "qpi_T = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_hidden]), name=\"qt/concentration\")))\n",
    "qpi_E = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_obs]), name=\"qe/concentration\")))\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # inference = ed.KLqp({model: qm}, {model: y_data})\n",
    "    inference_exp = ed.KLqp({pi_0: qpi_0,\n",
    "                         pi_T: qpi_T},\n",
    "                        {model: y_data, pi_E: qpi_E})\n",
    "    inference_max = ed.KLqp({pi_E: qpi_E},\n",
    "                        {model: y_data, pi_0: qpi_0, pi_T: qpi_T})\n",
    "    inference_exp.initialize()\n",
    "    inference_max.initialize()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        for _ in range(10):\n",
    "            dict_exp = inference_exp.update()\n",
    "        dict_max = inference_max.update()\n",
    "        if i % 100 == 0:\n",
    "            print(i, \" / 1000 done\")\n",
    "            # print(inference_exp.print_progress(dict_exp))\n",
    "            # print(inference_max.print_progress(dict_max))\n",
    "    inferred_pi0, inferred_pi_T, inferred_pi_E = sess.run([qpi_0.mean(), qpi_T.mean(), qpi_E.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45251185, 0.2563953 , 0.2910928 ],\n",
       "       [0.25287455, 0.6159876 , 0.13113792],\n",
       "       [0.21694666, 0.24478506, 0.53826827]], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24084574, 0.3120118 , 0.44714245],\n",
       "       [0.32083046, 0.3923618 , 0.28680775],\n",
       "       [0.67579246, 0.18118344, 0.14302412]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MCMC:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Empirical(params=tf.Variable(initial_value=tf.constant(1.0/n_hidden, shape=[T, n_hidden]))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails when HMC tries to convert some vars to unconstrained vars, I don't know why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'edward' from '/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/__init__.py'>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('simplex', 'points')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_0.support, qpi_0.support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Key-value pair in latent_vars does not have same shape: (3, 3), (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-e4300351fff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                    \u001b[0mpi_T\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqpi_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    pi_E: qpi_E},\n\u001b[0;32m---> 17\u001b[0;31m                                   data={model: y_data})\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# inference = ed.inferences.HMC({model: qm}, data={model: y_data})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/hmc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHMC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/monte_carlo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_vars, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m                            \"a scalar sample shape.\")\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonteCarlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/inference.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_vars, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mcheck_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py\u001b[0m in \u001b[0;36mcheck_latent_vars\u001b[0;34m(latent_vars)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       raise TypeError(\"Key-value pair in latent_vars does not have same \"\n\u001b[0;32m---> 81\u001b[0;31m                       \"shape: {}, {}\".format(key.shape, value.shape))\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       raise TypeError(\"Key-value pair in latent_vars does not have same \"\n",
      "\u001b[0;31mTypeError\u001b[0m: Key-value pair in latent_vars does not have same shape: (3, 3), (3,)"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "T = 5000 # number of MCMC samples\n",
    "\n",
    "qpi_0 = Empirical(params=tf.Variable(initial_value=tf.constant(1.0/n_hidden, shape=[T, n_hidden])))\n",
    "qpi_T = Empirical(params=tf.Variable(initial_value=tf.constant(1.0/n_hidden, shape=[T, n_hidden, n_hidden])))\n",
    "qpi_E = Empirical(params=tf.Variable(initial_value=tf.constant(1.0/n_obs, shape=[T, n_hidden, n_obs])))\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    inference = ed.inferences.HMC({pi_0: qpi_0,\n",
    "                                   pi_T: qpi_T,\n",
    "                                   pi_E: qpi_E},\n",
    "                                  data={model: y_data})\n",
    "    # inference = ed.inferences.HMC({model: qm}, data={model: y_data})\n",
    "    inference.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug: Problem when unconstraining pi_T (and therefore pi_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_unconstrained = ed.util.transform(pi_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz_unconstrained = qpi_E # because support = 'points', no need to be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.distributions.python.ops.bijectors.invert.Invert at 0x19e38ceb8>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bijectors.Invert(z_unconstrained.bijector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ed.RandomVariable 'Empirical_58/' shape=(3, 3) dtype=float32>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qz_unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2)])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_unconstrained.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(3)])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qz_unconstrained.event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if z_unconstrained != z: # it's the case since we transformed pi_0\n",
    "qz_constrained = ed.util.transform(qz_unconstrained, bijectors.Invert(z_unconstrained.bijector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(3)])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qpi_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qz_unconstrained.event_shape.ndims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bijectors.Invert(z_unconstrained.bijector)._inverse_event_shape(qz_unconstrained.event_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.models.TransformedDistribution(qz_unconstrained, bijectors.Invert(z_unconstrained.bijector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(Categorical(probs=Dirichlet(tf.ones([n_hidden, n_hidden]))).sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'HiddenMarkovModel_4/strided_slice:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 2 0 1 1 1 0 0 2 1 0 1 0 0 0 0 1 1 2 2 1 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(model.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import Categorical, Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from issue \n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(probs=[1., 0., 0.])\n",
    "# x_0 = Categorical(probs=tf.fill([n_hidden], 1.0 / n_hidden))\n",
    "\n",
    "# transition matrix\n",
    "# T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "T = np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.8], [0.1, 0.1, 0.8]])\n",
    "transition_distribution = Categorical(probs=T)\n",
    "\n",
    "# emission matrix\n",
    "# E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0)\n",
    "E = np.array(np.eye(3))\n",
    "emission_distribution = Categorical(probs=E)\n",
    "\n",
    "model = HiddenMarkovModel(\n",
    "    initial_distribution=x_0,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=emission_distribution,\n",
    "    num_steps=chain_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 2 1 1 1 1 0 0 1 1 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(model.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRICK TO USE TFP'S HMM AS A MM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from issue \n",
    "chain_len = 30\n",
    "n_states = 3\n",
    "\n",
    "pi_0 = Dirichlet(tf.ones(n_states))\n",
    "x_0 = Categorical(probs=pi_0)\n",
    "\n",
    "# transition matrix\n",
    "# pi_T = [Dirichlet(tf.ones(n_hidden)) for i in range(n_hidden)]\n",
    "pi_T = Dirichlet(tf.ones([n_states, n_states]))\n",
    "# T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "transition_distribution = Categorical(probs=pi_T)\n",
    "\n",
    "# emission matrix\n",
    "# E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0)\n",
    "pi_E = np.eye(n_states, dtype=np.float32) # identity matrix\n",
    "emission_distribution = Categorical(probs=pi_E)\n",
    "\n",
    "model = HiddenMarkovModel(\n",
    "    initial_distribution=x_0,\n",
    "    transition_distribution=transition_distribution,\n",
    "    observation_distribution=emission_distribution,\n",
    "    num_steps=chain_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: adding sample_shape to x_0, transmission_distribution, emission_distribution, and model, doesn't fail but doesn't converge either..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inference_4/sample_4/HiddenMarkovModel_4/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"data_4/Variable/read:0\", shape=(30,), dtype=int32)\n",
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 18s | Loss: 19.607\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qpi_0 = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden), name=\"q_0/concentration\")))\n",
    "qpi_T = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_hidden]), name=\"qt/concentration\")))\n",
    "\n",
    "# qm = HiddenMarkovModel(q_0, qt, qe, chain_len) # not necessary\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp({pi_0: qpi_0,\n",
    "                         pi_T: qpi_T},\n",
    "                        {model: y_data},)\n",
    "    inference.run(n_iter=5000, logdir='./testlog')\n",
    "    inferred_pi0, inferred_pi_T = sess.run([qpi_0.mean(), qpi_T.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71035326, 0.19993451, 0.08971225],\n",
       "       [0.08799313, 0.7047433 , 0.20726356],\n",
       "       [0.10449692, 0.11652052, 0.7789825 ]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inference_7/sample_7/HiddenMarkovModel_4/cond/Merge:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"data_7/Variable/read:0\", shape=(30,), dtype=int32)\n",
      "500/500 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 25.484\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qpi_0 = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden), name=\"q_0/concentration\")))\n",
    "qpi_T = Dirichlet(tf.nn.softplus(tf.Variable(tf.ones([n_hidden, n_hidden]), name=\"qt/concentration\")))\n",
    "\n",
    "# qm = HiddenMarkovModel(q_0, qt, qe, chain_len) # not necessary\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp({pi_0: qpi_0,\n",
    "                         pi_T: qpi_T},\n",
    "                        {model: y_data},)\n",
    "    inference.run(n_iter=500, logdir='./testlog')\n",
    "    inferred_pi0, inferred_pi_T = sess.run([qpi_0.mean(), qpi_T.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6771458 , 0.22621284, 0.09664141],\n",
       "       [0.159471  , 0.5947459 , 0.24578314],\n",
       "       [0.17056869, 0.06759135, 0.7618399 ]], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_pi_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
