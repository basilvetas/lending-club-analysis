{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model in Edward - mock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since good-examples was becoming messy I'm putting here different approaches to make inference work on a small Markov Model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO check out tips here: https://discourse.edwardlib.org/t/variational-em-for-independent-factor-analysis/61/2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Always use tf.nn.softmax or softplus when initializing the parameters of a Dirichlet to make sure that during the inference the params stay >0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "from pprint import pprint\n",
    "\n",
    "from edward.models import Categorical, Dirichlet, Uniform, Mixture\n",
    "from edward.models import Bernoulli, Normal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mock data used:** One trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "# for each categorical var y, he associated this matrix:\n",
    "np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Without Dirichlet Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if it works, not ideal because we do want to have priors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: [doesn't work] HMM, with Transition matrix + TF loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= initial code form Github issue: https://github.com/blei-lab/edward/issues/450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run it we get a similar error as the github issue (**but not exactly the same error as back then)**. It's because many of these objects are not instances of RandomVariable... If we dig in more into how Edward works we might understand why exactly and if there is anyway to avoid this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: [works and converges] HMM, with Transition matrix + Python loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-36a708141b0f>:9: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "# from issue \n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(probs=tf.fill([n_hidden], 1.0 / n_hidden))\n",
    "\n",
    "# transition matrix\n",
    "T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "\n",
    "# emission matrix\n",
    "E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_obs])), dim=0)\n",
    "\n",
    "# MODEL\n",
    "x = []\n",
    "y = []\n",
    "for _ in range(chain_len):\n",
    "    x_tm1 = x[-1] if x else x_0\n",
    "    x_t = Categorical(probs=T[x_tm1, :])\n",
    "    y_t = Categorical(probs=E[x_t, :])\n",
    "    x.append(x_t)\n",
    "    y.append(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4184508  0.34765527 0.342746  ]\n",
      " [0.34079152 0.23904131 0.27410343]\n",
      " [0.2407577  0.41330343 0.3831506 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 33s | Loss: 20.932\n",
      "[[0.7177787  0.11977834 0.71373457]\n",
      " [0.02416126 0.83216935 0.02445475]\n",
      " [0.2580601  0.04805234 0.26181066]]\n",
      "[[0.00221619 0.00382671 0.4800893 ]\n",
      " [0.9941004  0.9893936  0.0040356 ]\n",
      " [0.0036834  0.00677973 0.5158751 ]]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qx = [Categorical(probs=tf.nn.softmax(tf.Variable(tf.ones(n_hidden))))\n",
    "      for _ in range(chain_len)]\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = map(np.array, y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(T))\n",
    "\n",
    "    inference = ed.KLqp(dict(zip(x, qx)), dict(zip(y, y_data)))\n",
    "    inference.run(n_iter=5000)\n",
    "\n",
    "    inferred_T, inferred_E = sess.run([T, E])\n",
    "    inferred_qx = sess.run([foo.probs for foo in qx])\n",
    "    inferred_y_probs = sess.run([foo.probs for foo in y])\n",
    "    print(inferred_T)\n",
    "    print(inferred_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this example seems to converge to something better that whay the guy said in the github example. Also I switched column indexing to rows. **Sometimes it doesn't seem to actually converge. When it converges, it reaches a loss between 7 and 10**. 5K iterations seems to be enough. Less seemed not to converge but not sure if by re-running it wouldn't do better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given my 3 hidden states, 3 observation types, and long changes of identical observations, I expect transition matrix to be close to diagonal and the emission matrix to look like a permutation matrix. I see non-converging loss info. (the guy had a loss around 40, for 10K iterations)*, *non-uniform state probability distributions, and very uniform observation probabilities. Any idea what's going wrong in my setup or the solving of the problem?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitions: you almost always stay in the same state. Emission: you almost always go to the same state, but it doesn't have to be the same number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the external loop like this seems to work, fixing the length of the chains is not a big problem (anyway at some point LC stops the loan anyway so they cannot run indefinitely), we could do that while thinking of how to make it more efficient inside TF.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: [works but bad results] Regular MM, with Transition matrix + Python loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= Version 2 but without hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_len = 30\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(probs=tf.fill([n_obs], 1.0 / n_obs))\n",
    "\n",
    "# transition matrix\n",
    "T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_obs, n_obs])), dim=0)\n",
    "\n",
    "# no more emissions, we observe directly the hidden states x\n",
    "\n",
    "# MODEL\n",
    "x = []\n",
    "for _ in range(chain_len):\n",
    "    x_tm1 = x[-1] if x else x_0\n",
    "    x_t = Categorical(probs=T[:, x_tm1])\n",
    "    x.append(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 29s | Loss: 0.573\n",
      "[[0.13051206 0.1282776  0.12705421]\n",
      " [0.46439302 0.47969452 0.47204718]\n",
      " [0.40509495 0.39202783 0.40089864]]\n",
      "[array([0.13212071, 0.44575238, 0.4221269 ], dtype=float32), array([0.12733053, 0.5030093 , 0.3696602 ], dtype=float32), array([0.14275178, 0.46461993, 0.3926283 ], dtype=float32), array([0.14542097, 0.48163468, 0.3729444 ], dtype=float32), array([0.12822181, 0.46980274, 0.40197548], dtype=float32), array([0.12814322, 0.47672275, 0.39513403], dtype=float32), array([0.11452176, 0.5111241 , 0.37435412], dtype=float32), array([0.13028549, 0.43564564, 0.4340689 ], dtype=float32), array([0.14543663, 0.48284206, 0.37172136], dtype=float32), array([0.10846157, 0.50244874, 0.3890896 ], dtype=float32), array([0.10303085, 0.48524395, 0.41172528], dtype=float32), array([0.12696762, 0.4686965 , 0.4043359 ], dtype=float32), array([0.14359528, 0.4551472 , 0.4012575 ], dtype=float32), array([0.15214296, 0.45598698, 0.39187008], dtype=float32), array([0.1406253 , 0.45260644, 0.4067683 ], dtype=float32), array([0.12895179, 0.48353544, 0.3875128 ], dtype=float32), array([0.13589954, 0.45613128, 0.40796927], dtype=float32), array([0.1363262 , 0.47452083, 0.38915288], dtype=float32), array([0.11087236, 0.4784495 , 0.41067815], dtype=float32), array([0.11149119, 0.47826204, 0.41024682], dtype=float32), array([0.13388152, 0.5193051 , 0.34681338], dtype=float32), array([0.14028358, 0.4720483 , 0.38766816], dtype=float32), array([0.12791534, 0.47816133, 0.39392343], dtype=float32), array([0.12820181, 0.49056697, 0.38123122], dtype=float32), array([0.12757194, 0.50608456, 0.3663435 ], dtype=float32), array([0.11555569, 0.4885655 , 0.3958788 ], dtype=float32), array([0.12609737, 0.48273417, 0.3911684 ], dtype=float32), array([0.1424241 , 0.47502527, 0.38255066], dtype=float32), array([0.13231385, 0.45404866, 0.41363746], dtype=float32), array([0.13327223, 0.48096704, 0.38576072], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qx = [Categorical(probs=tf.nn.softmax(tf.Variable(tf.ones(n_obs))))\n",
    "      for _ in range(chain_len)]\n",
    "\n",
    "x_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "x_data = map(np.array, x_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "    # print(sess.run(T))\n",
    "\n",
    "    inference = ed.KLqp(dict(zip(x, qx)), dict(zip(x, x_data)))\n",
    "    inference.run(n_iter=5000)\n",
    "\n",
    "    inferred_T = sess.run(T)\n",
    "    inferred_qx = sess.run([foo.probs for foo in qx])\n",
    "    print(inferred_T)\n",
    "    print(inferred_qx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. With Dirichlet Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4: [works!] Regular MM + Python loop + Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(Dirichlet(tf.ones(n_hidden)))\n",
    "\n",
    "# transition matrix\n",
    "pi_T = [Dirichlet(tf.ones(n_hidden)) for i in range(n_hidden)]\n",
    "T = [Categorical(probs=pi) for pi in pi_T]\n",
    "\n",
    "# MODEL\n",
    "x = []\n",
    "for _ in range(chain_len):\n",
    "    x_tm1 = x[-1] if x else x_0\n",
    "    x_t = ed.models.Mixture(cat=Categorical(probs=tf.one_hot(x_tm1, n_hidden)), components=T)\n",
    "    x.append(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE (VI)**: WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 22s | Loss: 17.979\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "# qpi_T = [Dirichlet(tf.nn.softmax(tf.Variable(tf.ones(n_hidden)))) for i in range(n_hidden)]\n",
    "qpi_T = [Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden)))) for i in range(n_hidden)]\n",
    "\n",
    "latent_vars_map = dict(zip(pi_T, qpi_T))\n",
    "\n",
    "x_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "x_data = map(np.array, x_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp(latent_vars_map, dict(zip(x, x_data)))\n",
    "    inference.run(n_iter=5000)\n",
    "    inferred_qpi_T = sess.run([qpi.mean() for qpi in qpi_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.7036784, 0.1762671, 0.1200545], dtype=float32),\n",
       " array([0.10730021, 0.73918796, 0.15351184], dtype=float32),\n",
       " array([0.14591683, 0.08303422, 0.77104896], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_qpi_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE (MCMC)**: DOESN'T WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "conjugate_log_prob not implemented for <class 'abc.Mixture'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mget_log_joint\u001b[0;34m(cond_set)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# Use log prob tensor if already built in graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mconjugate_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3653\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3654\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3519\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3520\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3521\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'conjugate_log_joint/Mixture_17/_conjugate_log_prob:0' refers to a Tensor which does not exist. The operation, 'conjugate_log_joint/Mixture_17/_conjugate_log_prob', does not exist in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b883c965b800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# inference = ed.inferences.MetropolisHastings(latent_vars_map, dict(zip(y, y_data)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vars_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0minferred_qpi_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mqpi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqpi_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/gibbs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_vars, proposal_vars, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproposal_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       proposal_vars = {z: complete_conditional(z)\n\u001b[0;32m---> 46\u001b[0;31m                        for z in six.iterkeys(latent_vars)}\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mcheck_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposal_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/gibbs.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproposal_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       proposal_vars = {z: complete_conditional(z)\n\u001b[0;32m---> 46\u001b[0;31m                        for z in six.iterkeys(latent_vars)}\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mcheck_latent_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposal_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mcomplete_conditional\u001b[0;34m(rv, cond_set)\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete_conditional_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# log_joint holds all the information we need to get a conditional.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mlog_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Pull out the nodes that are nonlinear functions of rv into s_stats.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/conjugacy/conjugacy.py\u001b[0m in \u001b[0;36mget_log_joint\u001b[0;34m(cond_set)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"conjugate_log_prob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m           raise NotImplementedError(\"conjugate_log_prob not implemented for\"\n\u001b[0;32m--> 190\u001b[0;31m                                     \" {}\".format(type(b)))\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mconjugate_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: conjugate_log_prob not implemented for <class 'abc.Mixture'>"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "T = 5000 # number of MCMC samples\n",
    "\n",
    "# Maybe this is not the right way to initialize:\n",
    "qpi_T = [ed.models.Empirical(\n",
    "    tf.Variable(expected_shape=[n_hidden],\n",
    "                initial_value=tf.constant(1.0/n_hidden, shape=[T, n_hidden]))) for i in range(n_hidden)]\n",
    "\n",
    "latent_vars_map = dict(zip(pi_T, qpi_T))\n",
    "\n",
    "x_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "x_data = map(np.array, x_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # inference = ed.inferences.MetropolisHastings(latent_vars_map, dict(zip(y, y_data)))\n",
    "    inference = ed.inferences.Gibbs(latent_vars_map, data=dict(zip(x, x_data)))\n",
    "    inference.run()\n",
    "    inferred_qpi_T = sess.run([qpi.mean() for qpi in qpi_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.333344, 0.333344, 0.333344], dtype=float32),\n",
       " array([0.333344, 0.333344, 0.333344], dtype=float32),\n",
       " array([0.333344, 0.333344, 0.333344], dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_qpi_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 5: [don't know if works] HMM, without Transition matrix + Dirichlet + Python loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Edward seems to be designed to know when to do EM when necessary, here we need to do EM because we don't observe the hidden state, we need to look more into that and see if it will figure it out or if we need to do it ourselves (instantiate two inference objects, and run alternatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(Dirichlet(tf.ones(n_hidden)))\n",
    "\n",
    "# transition matrix\n",
    "pi_T = [Dirichlet(tf.ones(n_hidden)) for i in range(n_hidden)]\n",
    "T = [Categorical(probs=pi) for pi in pi_T]\n",
    "\n",
    "# emission matrix\n",
    "pi_E = [Dirichlet(tf.ones(n_obs)) for i in range(n_obs)]\n",
    "E = [Categorical(probs=pi) for pi in pi_E]\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for _ in range(chain_len):\n",
    "    x_tm1 = x[-1] if x else x_0\n",
    "    x_t = ed.models.Mixture(cat=Categorical(probs=tf.one_hot(x_tm1, n_hidden)), components=T)\n",
    "    y_t = ed.models.Mixture(cat=Categorical(probs=tf.one_hot(x_t, n_hidden)), components=E)\n",
    "    x.append(x_t)\n",
    "    y.append(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE (VI) ON BOTH QPIT and QPIE** RESULTS NOT THAT GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 43s | Loss: 37.294\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qpi_T = [Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden)))) for i in range(n_hidden)]\n",
    "qpi_E = [Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_obs)))) for i in range(n_hidden)]\n",
    "\n",
    "latent_vars_map = dict(zip(pi_T, qpi_T))\n",
    "latent_vars_map.update(dict(zip(pi_E, qpi_E)))\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = map(np.array, y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp(latent_vars_map, data=dict(zip(y, y_data)))\n",
    "    inference.run(n_iter=5000)\n",
    "    inferred_qpi_T = sess.run([qpi.mean() for qpi in qpi_T])\n",
    "    inferred_qpi_E = sess.run([qpi.mean() for qpi in qpi_E])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.5163122 , 0.19137841, 0.29230946], dtype=float32),\n",
       " array([0.4316325 , 0.20450503, 0.3638625 ], dtype=float32),\n",
       " array([0.2909255 , 0.18665254, 0.522422  ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_qpi_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.35055286, 0.38690096, 0.26254615], dtype=float32),\n",
       " array([0.31107655, 0.37114182, 0.31778166], dtype=float32),\n",
       " array([0.45351365, 0.251756  , 0.29473028], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_qpi_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INFERENCE (VI) ON ONLY QPIT:** RESULTS NOT THAT GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 43s | Loss: 44.499\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qpi_T = [Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_hidden)))) for i in range(n_hidden)]\n",
    "# qpi_E = [Dirichlet(tf.nn.softplus(tf.Variable(tf.ones(n_obs)))) for i in range(n_hidden)]\n",
    "\n",
    "latent_vars_map = dict(zip(pi_T, qpi_T))\n",
    "# latent_vars_map.update(dict(zip(pi_E, qpi_E)))\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = map(np.array, y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inference = ed.KLqp(latent_vars_map, data=dict(zip(y, y_data)))\n",
    "    inference.run(n_iter=5000)\n",
    "    inferred_qpi_T = sess.run([qpi.mean() for qpi in qpi_T])\n",
    "    # inferred_qpi_E = sess.run([qpi.mean() for qpi in qpi_E])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.2658316 , 0.440889  , 0.29327947], dtype=float32),\n",
       " array([0.5744874 , 0.08134261, 0.34417003], dtype=float32),\n",
       " array([0.2542029 , 0.46512648, 0.2806706 ], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_qpi_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Other tutorials/ideas we might consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation of HMM but not suited for inference: https://gist.github.com/fredcallaway/c7252b6326dfb502e70cad4146731aef\n",
    "- Implementation of HMM by creating a custom RV class: might work but needs a lot of work, the code doesn't run as is: https://discourse.edwardlib.org/t/hmm-implementation-with-marginalized-latent-states/755\n",
    "- Using a list of categoricals and then using tf.gather to select from this list: Doesn't work, because tf.gather makes us loose the type so we no longer have Categorical variables => can't run inference (we can still sample though)\n",
    "- Using the OneHotCategorical RV instead of a Categorical RV and then taking tf.one_hot of it didn't seem to change much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
