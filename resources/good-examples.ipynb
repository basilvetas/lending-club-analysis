{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good examples and tips in Edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code that works and is useful, that I didn't want to throw away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi: [0.24454049 0.46987182 0.13792581 0.14766188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 43.773\n",
      "Inferred pi: [0.29658943 0.33174667 0.22883385 0.14283012]\n"
     ]
    }
   ],
   "source": [
    "# code from edward/examples/dirichlet_categorical.py:\n",
    "# Inferring a categorical distribution with KLqp\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "K = 4\n",
    "N = 30\n",
    "# DATA\n",
    "pi_true = np.random.dirichlet(np.array([20.0, 30.0, 10.0, 10.0]))\n",
    "z_data = np.array([np.random.choice(K, 1, p=pi_true)[0]\n",
    "                   for n in range(N)])\n",
    "print(\"pi: {}\".format(pi_true))\n",
    "\n",
    "# MODEL\n",
    "pi = ed.models.Dirichlet(tf.ones(4))\n",
    "z = ed.models.Categorical(probs=pi, sample_shape=N)\n",
    "\n",
    "# INFERENCE\n",
    "qpi = ed.models.Dirichlet(tf.nn.softplus(\n",
    "    tf.get_variable(\"qpi/concentration\", [K])))\n",
    "\n",
    "inference = ed.KLqp({pi: qpi}, data={z: z_data})\n",
    "inference.run(n_iter=1500, n_samples=30)\n",
    "\n",
    "# sess = ed.get_session()\n",
    "print(\"Inferred pi: {}\".format(sess.run(qpi.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  \"\"\"\n",
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 8s | Acceptance Rate: 1.000\n"
     ]
    }
   ],
   "source": [
    "# example from examples/bayesian_linear_regression_sghmc.py\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "def build_toy_dataset(N, noise_std=0.5):\n",
    "    X = np.concatenate([np.linspace(0, 2, num=N / 2),\n",
    "                        np.linspace(6, 8, num=N / 2)])\n",
    "    y = 2.0 * X + 10 * np.random.normal(0, noise_std, size=N)\n",
    "    X = X.astype(np.float32).reshape((N, 1))\n",
    "    y = y.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 1  # number of features\n",
    "# DATA\n",
    "X_train, y_train = build_toy_dataset(N)\n",
    "X_test, y_test = build_toy_dataset(N)\n",
    "\n",
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "w = ed.models.Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = ed.models.Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = ed.models.Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N))\n",
    "\n",
    "# INFERENCE\n",
    "T = 5000                        # Number of samples.\n",
    "nburn = 100                     # Number of burn-in samples.\n",
    "stride = 10                     # Frequency with which to plot samples.\n",
    "qw = ed.models.Empirical(params=tf.Variable(tf.random_normal([T, D])))\n",
    "qb = ed.models.Empirical(params=tf.Variable(tf.random_normal([T, 1])))\n",
    "\n",
    "inference = ed.SGHMC({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(step_size=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Markov Chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second attempt, seems to work now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from issue https://github.com/blei-lab/edward/issues/450\n",
    "from edward.models import Categorical, Dirichlet, Uniform, Mixture\n",
    "\n",
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "\n",
    "x_0 = Categorical(probs=tf.fill([n_hidden], 1.0 / n_hidden))\n",
    "\n",
    "# transition matrix\n",
    "T = tf.nn.softmax(tf.Variable(tf.random_uniform([n_hidden, n_hidden])), dim=0)\n",
    "\n",
    "# emission matrix\n",
    "E = tf.nn.softmax(tf.Variable(tf.random_uniform([n_obs, n_hidden])), dim=0)\n",
    "\n",
    "# MODEL\n",
    "x = []\n",
    "y = []\n",
    "for _ in range(chain_len):\n",
    "    x_tm1 = x[-1] if x else x_0\n",
    "    x_t = Categorical(probs=T[:, x_tm1])\n",
    "    y_t = Categorical(probs=E[:, x_t])\n",
    "    x.append(x_t)\n",
    "    y.append(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42720532 0.37529522 0.29069752]\n",
      " [0.20214134 0.34706113 0.3367309 ]\n",
      " [0.3706533  0.27764365 0.3725716 ]]\n",
      "20000/20000 [100%] ██████████████████████████████ Elapsed: 107s | Loss: 10.563\n",
      "[[0.8985785  0.03703917 0.03406724]\n",
      " [0.00348154 0.9607762  0.09740151]\n",
      " [0.09793993 0.00218464 0.8685312 ]]\n",
      "[[9.9671930e-01 1.9515709e-03 2.1848876e-03]\n",
      " [1.7951096e-03 1.2189067e-03 9.9692792e-01]\n",
      " [1.4855342e-03 9.9682957e-01 8.8716898e-04]]\n",
      "[array([0.9960776 , 0.0024412 , 0.00148117], dtype=float32), array([9.9728549e-01, 1.8385181e-03, 8.7593711e-04], dtype=float32), array([9.9807847e-01, 1.0312266e-03, 8.9030614e-04], dtype=float32), array([0.9966865 , 0.0017852 , 0.00152831], dtype=float32), array([0.9966016 , 0.0014918 , 0.00190652], dtype=float32), array([9.9790359e-01, 9.5059699e-04, 1.1458463e-03], dtype=float32), array([0.9974504 , 0.00139323, 0.0011563 ], dtype=float32), array([0.9974974 , 0.00111923, 0.00138342], dtype=float32), array([9.9841952e-01, 8.1004773e-04, 7.7040045e-04], dtype=float32), array([0.99220324, 0.00111062, 0.00668607], dtype=float32), array([0.00482394, 0.00125331, 0.99392277], dtype=float32), array([1.0225778e-03, 9.2023355e-04, 9.9805719e-01], dtype=float32), array([0.00117362, 0.00113399, 0.99769247], dtype=float32), array([7.7024673e-04, 1.9842167e-04, 9.9903131e-01], dtype=float32), array([0.0013318 , 0.00104346, 0.9976247 ], dtype=float32), array([6.5106561e-04, 4.1685646e-04, 9.9893206e-01], dtype=float32), array([1.0831788e-03, 3.4776086e-04, 9.9856901e-01], dtype=float32), array([2.273914e-03, 6.358411e-04, 9.970902e-01], dtype=float32), array([9.0079213e-04, 6.2336837e-04, 9.9847585e-01], dtype=float32), array([0.00168717, 0.0027686 , 0.99554425], dtype=float32), array([9.1571111e-04, 9.9745685e-01, 1.6274209e-03], dtype=float32), array([5.247183e-04, 9.986305e-01, 8.447890e-04], dtype=float32), array([5.9271744e-04, 9.9852008e-01, 8.8727201e-04], dtype=float32), array([7.0620701e-04, 9.9896073e-01, 3.3305865e-04], dtype=float32), array([5.9400854e-04, 9.9880111e-01, 6.0480332e-04], dtype=float32), array([8.306088e-04, 9.986512e-01, 5.181944e-04], dtype=float32), array([7.5984187e-04, 9.9876910e-01, 4.7102777e-04], dtype=float32), array([1.0499307e-03, 9.9861681e-01, 3.3322000e-04], dtype=float32), array([9.1070356e-04, 9.9853826e-01, 5.5101799e-04], dtype=float32), array([1.3592112e-03, 9.9826944e-01, 3.7133825e-04], dtype=float32)]\n",
      "[array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([0.9967193 , 0.00179511, 0.00148553], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32), array([2.1848876e-03, 9.9692792e-01, 8.8716898e-04], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "qx = [Categorical(probs=tf.nn.softmax(tf.Variable(tf.ones(n_hidden))))\n",
    "      for _ in range(chain_len)]\n",
    "\n",
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "y_data = map(np.array, y_data)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(T))\n",
    "\n",
    "    inference = ed.KLqp(dict(zip(x, qx)), dict(zip(y, y_data)))\n",
    "    inference.run(n_iter=20000)\n",
    "\n",
    "    print(sess.run(T))\n",
    "    print(sess.run(E))\n",
    "    print(sess.run([foo.probs for foo in qx]))\n",
    "    print(sess.run([foo.probs for foo in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this example seems to converge to something better that whay the guy said in the github example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Given my 3 hidden states, 3 observation types, and long changes of identical observations, I expect transition matrix to be close to diagonal and the emission matrix to look like a permutation matrix. I see non-converging loss info:*\n",
    "\n",
    "Iteration     1 [  0%]: Loss = 35.123\n",
    "Iteration  2000 [ 10%]: Loss = 46.565\n",
    "Iteration  4000 [ 20%]: Loss = 47.923\n",
    "Iteration  6000 [ 30%]: Loss = 56.581\n",
    "Iteration  8000 [ 40%]: Loss = 53.431\n",
    "Iteration 10000 [ 50%]: Loss = 49.397\n",
    "Iteration 12000 [ 60%]: Loss = 55.551\n",
    "Iteration 14000 [ 70%]: Loss = 47.537\n",
    "Iteration 16000 [ 80%]: Loss = 47.690\n",
    "Iteration 18000 [ 90%]: Loss = 47.107\n",
    "Iteration 20000 [100%]: Loss = 38.514\n",
    "\n",
    "*, non-uniform state probability distributions, and very uniform observation probabilities. Any idea what's going wrong in my setup or the solving of the problem?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What data did we pass in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = ([0] * 10) + ([1] * 10) + ([2] * 10)\n",
    "# for each categorical var y, he associated this matrix:\n",
    "np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitions: you almost always stay in the same state. Emission: you almost always go to the same state, but it doesn't have to be the same number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the external loop like this seems to work, fixing the length of the chains is not a big problem (anyway at some point LC stops the loan anyway so they cannot run indefinitely), we could do that while thinking of how to make it more efficient inside TF.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First attempt:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_len = 30\n",
    "n_hidden = 3\n",
    "n_obs = 3\n",
    "x_0 = Categorical(probs=tf.fill([n_hidden], 1.0 / n_hidden))\n",
    "\n",
    "# transition matrix\n",
    "T = Uniform(tf.zeros([n_hidden, n_hidden]), tf.ones([n_hidden, n_hidden]))\n",
    "T /= tf.reduce_sum(T, axis=0, keep_dims=True)\n",
    "\n",
    "# emission matrix\n",
    "E = Uniform(tf.zeros([n_obs, n_hidden]), tf.ones([n_obs, n_hidden]))\n",
    "E /= tf.reduce_sum(E, axis=0, keep_dims=True)\n",
    "\n",
    "# model\n",
    "y_val = tf.placeholder(tf.int32, [chain_len])\n",
    "x = tf.scan(lambda x_tm1, _: Categorical(probs=T[:, x_tm1]),\n",
    "            y_val, initializer=x_0)\n",
    "y = tf.map_fn(lambda x_t: Categorical(probs=E[:, x_t]), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'reparameterization_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1c7e792d42b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/inference.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, variables, use_coordinator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mPassed\u001b[0m \u001b[0minto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, n_samples, kl_scaling, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_scaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKLqp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/variational_inference.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, optimizer, var_list, use_prettytensor, global_step, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36mbuild_loss_and_gradients\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterization_type\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFULLY_REPARAMETERIZED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         for rv in six.itervalues(self.latent_vars)])\n\u001b[0m\u001b[1;32m    139\u001b[0m     is_analytic_kl = all([isinstance(z, Normal) and isinstance(qz, Normal)\n\u001b[1;32m    140\u001b[0m                           for z, qz in six.iteritems(self.latent_vars)])\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.1/envs/prob-prog/lib/python3.6/site-packages/edward/inferences/klqp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterization_type\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFULLY_REPARAMETERIZED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         for rv in six.itervalues(self.latent_vars)])\n\u001b[0m\u001b[1;32m    139\u001b[0m     is_analytic_kl = all([isinstance(z, Normal) and isinstance(qz, Normal)\n\u001b[1;32m    140\u001b[0m                           for z, qz in six.iteritems(self.latent_vars)])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'reparameterization_type'"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "qT = tf.Variable(Uniform(tf.zeros([n_hidden, n_hidden]), tf.ones([n_hidden, n_hidden])))\n",
    "qT /= tf.reduce_sum(qT, axis=0, keep_dims=True)\n",
    "qE = tf.Variable(Uniform(tf.zeros([n_obs, n_hidden]), tf.ones([n_obs, n_hidden])))\n",
    "qE /= tf.reduce_sum(qE, axis=0, keep_dims=True)\n",
    "px_init = tf.Variable(tf.random_uniform([chain_len, n_hidden]))\n",
    "px_init /= tf.reduce_sum(px_init, axis=0, keep_dims=True)\n",
    "qx = Categorical(probs=px_init)\n",
    "\n",
    "y_data = np.array(([0] * 10) + ([1] * 10) + ([2] * 10), dtype=np.int32)\n",
    "\n",
    "inference = ed.KLqp({T: qT, E: qE, x: qx}, {y_val: y_data})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems similar to the github issue (**but not exactly the same error as back then)**. It's because many of these objects are not instances of RandomVariable... If we dig in more into how Edward works we might understand why exactly and if there is anyway to avoid this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
